{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addPedestrian(in_file, out_file, ped_id, x, y, target_ids, speed):\n",
    "    ped = {\n",
    "      \"source\" : None,\n",
    "      \"targetIds\" : target_ids,\n",
    "      \"position\" : {\n",
    "        \"x\" : x,\n",
    "        \"y\" : y\n",
    "      },\n",
    "      \"velocity\" : {\n",
    "        \"x\" : 0.0,\n",
    "        \"y\" : 0.0\n",
    "      },\n",
    "      \"nextTargetListIndex\" : 0,\n",
    "      \"freeFlowSpeed\" : speed,\n",
    "      \"attributes\" : {\n",
    "        \"id\" : ped_id,\n",
    "        \"radius\" : 0.2,\n",
    "        \"densityDependentSpeed\" : False,\n",
    "        \"speedDistributionMean\" : 1.34,\n",
    "        \"speedDistributionStandardDeviation\" : 0.26,\n",
    "        \"minimumSpeed\" : 0.5,\n",
    "        \"maximumSpeed\" : 2.2,\n",
    "        \"acceleration\" : 2.0,\n",
    "        \"footStepsToStore\" : 4,\n",
    "        \"searchRadius\" : 1.0,\n",
    "        \"angleCalculationType\" : \"USE_CENTER\",\n",
    "        \"targetOrientationAngleThreshold\" : 45.0\n",
    "      },\n",
    "      \"idAsTarget\" : -1,\n",
    "      \"isChild\" : False,\n",
    "      \"isLikelyInjured\" : False,\n",
    "      \"mostImportantEvent\" : None,\n",
    "      \"salientBehavior\" : \"TARGET_ORIENTED\",\n",
    "      \"groupIds\" : [ ],\n",
    "      \"trajectory\" : {\n",
    "        \"footSteps\" : [ ]\n",
    "      },\n",
    "      \"groupSizes\" : [ ],\n",
    "      \"modelPedestrianMap\" : { },\n",
    "      \"type\" : \"PEDESTRIAN\"\n",
    "    }\n",
    "    with open('./scenarios/' + in_file + '.scenario', 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "        data['name'] = out_file\n",
    "        data['scenario']['topography']['dynamicElements'].append(ped)\n",
    "    with open('./scenarios/' + out_file + '.scenario', 'w') as outfile:\n",
    "        json.dump(data, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scenarios\n",
    "def create_scenario(scenario_name, p_xy, p_v):\n",
    "    with open('./scenarios/bottleneck_gnm.scenario', 'r') as infile:\n",
    "        data = json.load(infile)\n",
    "    with open('./scenarios/' + scenario_name + '.scenario', 'w') as new_sec:\n",
    "        json.dump(data, new_sec, indent=2)\n",
    "    peds = p_xy.reshape((-1,2))\n",
    "    p_id = 1\n",
    "    for p in peds:\n",
    "        p_x = p[0]\n",
    "        p_y = p[1]\n",
    "        addPedestrian(scenario_name, scenario_name, p_id, p_x, p_y, [2], p_v[p_id-1])\n",
    "        p_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running scenarios\n",
    "# !!! vadere-console.jar must be placed in the same folder as this notebook!!!\n",
    "def run_scenario(scenario_name):\n",
    "    java_command = 'java -jar vadere-console.jar scenario-run'\n",
    "    command = java_command + ' --scenario-file \"scenarios/' + scenario_name + '.scenario\" --output-dir=\"output\"'\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading OSM\n",
    "def load_postvis(sec_name):\n",
    "    filename = './output/' + sec_name + '/postvis.trajectories'\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        next(reader)\n",
    "        data = []\n",
    "        for row in reader:\n",
    "            pedestrian = [int(row[0]), int(row[1]), float(row[2]), float(row[3]), int(row[4]), float(row[5])]\n",
    "            # After step 29 the first pedestrian \"disappears\" inside the target\n",
    "            if (int(row[0]) > 29): continue\n",
    "            a = np.array(pedestrian)\n",
    "            data.append(a)\n",
    "        return np.row_stack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not required for the tasks, but used for testing\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta Information\n",
    "osm_sec_name = 'bottleneck_osm_' + str(n)\n",
    "matrix = load_postvis(osm_sec_name)\n",
    "NT = int(max(matrix[:,0]))\n",
    "m = 10\n",
    "d = 2\n",
    "nd = n*d\n",
    "\n",
    "true_error = 1e-4\n",
    "\n",
    "random_seed = 1\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initializing xt\n",
    "xt = np.zeros((NT, (nd*m)))\n",
    "# Pedestrian velocities\n",
    "p_v = np.zeros((NT,n))\n",
    "for r in matrix:\n",
    "    for i in range(m):\n",
    "        step = int(r[0]) - 1\n",
    "        p_id = int(r[1]) - 1\n",
    "        p_x = r[2] + np.random.normal(0,1)\n",
    "        p_y = r[3] + np.random.normal(0,1)\n",
    "        p_v[step,p_id] = r[5]\n",
    "        xt[step,(p_id*d+i*nd):((p_id+1)*d+i*nd)] = np.array([p_x, p_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normal_draw(cov):\n",
    "    \"\"\" draw an n-dimensional point from a Gaussian distribution with given covariance. \"\"\"\n",
    "    return np.random.multivariate_normal(0*cov[:,0],cov,n)\n",
    "\n",
    "def observation(x):\n",
    "    return x\n",
    "\n",
    "def f_model(p_xy, ts):\n",
    "    sec_name = 'f_hat'\n",
    "    create_scenario(sec_name, p_xy, p_v[ts])\n",
    "    run_scenario(sec_name)\n",
    "    matrix = load_postvis(sec_name)\n",
    "    result = np.zeros((nd,))\n",
    "    for r in matrix:\n",
    "        step = int(r[0]) - 1\n",
    "        if (step == 0): continue\n",
    "        p_id = int(r[1]) - 1\n",
    "        p_x = r[2]\n",
    "        p_y = r[3]\n",
    "        result[(p_id*d):((p_id+1)*d)] = np.array([p_x, p_y])\n",
    "    return result    \n",
    "\n",
    "def enks(z_data, error_covariance_M, error_covariance_Q, observation, fhat_model):\n",
    "    t = z_data.shape[0]\n",
    "    ML = (error_covariance_M)\n",
    "    QL = (error_covariance_Q)\n",
    "    # initialize the initial guess for the model state with random numbers \n",
    "    xk = np.random.rand(z_data.shape[0],z_data.shape[1])+2\n",
    "    \n",
    "    for k in range(1,t):\n",
    "        zk_hat = np.zeros((z_data.shape[1],))\n",
    "        zk_sum = np.zeros(nd)\n",
    "        for i in range(m):\n",
    "            mkm1 = normal_draw(ML)\n",
    "            xk[k,(i*nd):((i+1)*nd)] = fhat_model(xk[k-1,(i*nd):((i+1)*nd)].reshape(1,-1),k-1) + mkm1.reshape(1,-1)\n",
    "            qk = normal_draw(QL)\n",
    "            zk_hat[(i*nd):((i+1)*nd)] = observation(xk[k,(i*nd):((i+1)*nd)].reshape(1,-1)) + qk.reshape(1, -1)\n",
    "            zk_sum += zk_hat[(i*nd):((i+1)*nd)]\n",
    "            \n",
    "        zk_bar = (1/m)*zk_sum\n",
    "        \n",
    "        zk_outer_sum = np.zeros((nd, nd))\n",
    "        for i in range(m):\n",
    "            zkdiff = zk_hat[(i*nd):((i+1)*nd)] - zk_bar\n",
    "            outer = np.outer(zkdiff, zkdiff)\n",
    "            zk_outer_sum += outer\n",
    "        \n",
    "        Zk = (1/m)*zk_outer_sum\n",
    "        \n",
    "        for j in range(1,k+1):\n",
    "            # xjbar\n",
    "            xj_hat_sum = np.zeros(nd)\n",
    "            for i in range(m):\n",
    "                xj_hat_sum += xk[j,(i*nd):((i+1)*nd)]\n",
    "            xj_bar = (1/m)*xj_hat_sum\n",
    "            \n",
    "            # sigmaj\n",
    "            outer_sum = np.zeros((nd,nd))\n",
    "            for i in range(m):\n",
    "                xj_diff = xk[j,(i*nd):((i+1)*nd)] - xj_bar\n",
    "                zk_diff = zk_hat[(i*nd):((i+1)*nd)] - zk_bar\n",
    "                outer_sum += np.outer(xj_diff, zk_diff)\n",
    "            sigmaj = (1/(m-1))*outer_sum\n",
    "            \n",
    "            # reassigning x_hat\n",
    "            Zk_inv = np.linalg.pinv(Zk,rcond=1e-10)\n",
    "            mat_prod = sigmaj.dot(Zk_inv)\n",
    "            for i in range(m):\n",
    "                zk_diff = z_data[k,(i*nd):((i+1)*nd)] - zk_hat[(i*nd):((i+1)*nd)]\n",
    "                xk[j,(i*nd):((i+1)*nd)] = xk[j,(i*nd):((i+1)*nd)] + mat_prod.dot(zk_diff)\n",
    "            \n",
    "    return xk\n",
    "\n",
    "def max_likelihood(xk, fhat_model):\n",
    "    t = xk.shape[0]\n",
    "    \n",
    "    M_new = np.zeros((d,d))\n",
    "    for k in range(1, t-1):\n",
    "        for i in range(m):\n",
    "            fhat = fhat_model(xk[k,(i*nd):((i+1)*nd)].reshape(1,-1),k)\n",
    "            xhat = xk[k+1,(i*nd):((i+1)*nd)]\n",
    "            for j in range(n):\n",
    "                fhatj = fhat[(j*d):((j+1)*d)]\n",
    "                xhatj = xhat[(j*d):((j+1)*d)]\n",
    "                sub = xhatj-fhatj\n",
    "                outer = np.outer(sub, sub)\n",
    "                M_new += outer\n",
    "    M_new /= t*m*n\n",
    "    return M_new\n",
    "\n",
    "\n",
    "# this is the initial guess for the entropy matrix. can be pretty arbitrary\n",
    "M = np.ones((d,d))\n",
    "# this is the guess for the true error in the observations. should be small here.\n",
    "Q = M * true_error**2\n",
    "\n",
    "N_ITER = 5 # number of iterations of algorithm1_enks and max_likelihood \n",
    "Mhat = M\n",
    "zk = observation(xt[:,:])\n",
    "xm_hat = 0\n",
    "xm_hat_prev = 0\n",
    "for k in range(N_ITER):\n",
    "    xm_hat = enks(zk, Mhat, Q, observation, lambda x,y: f_model(x,y));\n",
    "    Mhat = max_likelihood(xm_hat, lambda x,y: f_model(x,y)) \n",
    "    print('current det(M)', np.linalg.det(Mhat))\n",
    "    print('error change ', np.linalg.norm(xm_hat - xm_hat_prev)) \n",
    "    xm_hat_prev = xm_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(M):\n",
    "    return 1/2 * n * np.log((2*np.pi*np.exp(1))**d * np.linalg.det(M))\n",
    "print('entropy(M estimated) ', entropy(Mhat)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
